{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer vision workshop\n",
    "\n",
    "Welcome to this workshop on Computer Vision and Convolutional Neural Networks. In a couple of steps we explain how computer vision techniques can be used to manipulate and analyse images and how convolutional neural networks use transformations to analyse visual material.\n",
    "\n",
    "## Instructions\n",
    "Prior knowledge of Python (a computer language) is not needed, but will help you understand this notebook better. The same goes for Jupyter Notebooks. One important thing to understand: a Jupyter notebook consists of cells that can  contain blocks of code or text ('markdown'). For example, this block contains markdown only. In cells that contain code, we can add text as comments using the '#' sign. This text is used to explain code. Sometimes we ask a question after an '#' sign. If you want to write your answer down in the same cell, use an # to signify it as text. \n",
    "\n",
    "In order to make the code work you need to run code in the cells in a top to bottom order. You do this by pressing 'shift + enter', which will automatically take you to the next cell. Try it now!\n",
    "\n",
    "NB: This notebook builds upon earlier notebooks/work by <br>\n",
    "<a href=\"https://github.com/SeguinBe/DHWorkshop2017\" /> Benoit Séguin </a> <br>\n",
    "<a href=\"https://distantviewing.org/tutorial-dl\" /> Taylor Arnold & Lauren Tilton </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by loading several libraries we need to perform the tasks in this notebook. \n",
    "# Dont forget to press 'Shift + enter'\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import skimage\n",
    "from skimage import data, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preloading models\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "vgg19_full = VGG19(weights='imagenet')\n",
    "\n",
    "from keras.applications import inception_v3\n",
    "inception_model = inception_v3.InceptionV3(include_top=False, pooling='avg', weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images and Python\n",
    "\n",
    "We start by loading an image in the notebook using Python: in this case, the image with the filename 'color_train.jpg' from the folder 'images' (see the code below). `plt.imshow(img)` shows the image in the notebook in a grid, which shows the number of pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load an image using skimage. \n",
    "img = io.imread(join(\"images\", \"color_train.jpg\"))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixels\n",
    "Computers do not see images, but only a large number of pixel values. Each pixel has a value between 0 and 255. As a result, computers are ('only') able to analyse patterns in pixels values. \n",
    "<br><br><br><img src=\"https://openframeworks.cc/ofBook/images/image_processing_computer_vision/images/lincoln_pixel_values.png\" /><p style=\"text-align:center;\">\n",
    "Pixel data diagram. At left, our image of Lincoln; at center, the pixels labeled with numbers from 0-255, representing their brightness; and at right, these numbers by themselves.\n",
    "</p>\n",
    "<p style=\"text-align:center;\">\n",
    "(borrowed from Golan Levin [Image Processing and Computer Vision](https://openframeworks.cc/ofBook/chapters/image_processing_computer_vision.html))</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use 'img.shape' to print information on the number of pixels.\n",
    "# QUESTION: What do the numbers stand for? Look at the image, and the grid pattern, in the image in cell 6. \n",
    "# QUESTION: What does the 3 mean? \n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An image can consist of three color layers: red, green, and blue, all with their own pixel values.\n",
    "\n",
    "img_rgb = io.imread(join(\"images\", \"iur.png\"))\n",
    "plt.imshow(img_rgb)\n",
    "\n",
    "red = img_rgb[:,:,0] #select the first channel and save into variable\n",
    "green = img_rgb[:,:,1]\n",
    "blue = img_rgb[:,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,3,1) #plot one figure with 1 row and 3 columns of images.\n",
    "plt.title('Red')\n",
    "plt.imshow(red, cmap='gray')\n",
    "plt.axis('off') # plot no axis\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('Green')\n",
    "plt.imshow(green, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('Blue')\n",
    "plt.imshow(blue, cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "Because computers see images as collections of numbers, we can use math to transform the pixel values and, as a result, manipulate the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary library\n",
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will rescale the image by a factor 0.5 (divide the number of pixels on the X and Y axel by 2)\n",
    "# QUESTION: The size of the displayed image did not change. Why? What did change? \n",
    "rescaled_img = transform.rescale(img, scale=0.5, mode='constant', multichannel=True, anti_aliasing=True)\n",
    "plt.imshow(rescaled_img)\n",
    "rescaled_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will resize the image to have a size of 200 x 200 pixels. \n",
    "# Before applying Convolutional Neural Networks, images are often 'preprocessed' in this, and other, ways.\n",
    "# QUESTION: resizing the image in this way fundamentally changes our perception of the image. \n",
    "# Is this the same for a computer?\n",
    "resized_img = transform.resize(img, (200,200), mode='constant', anti_aliasing=True)\n",
    "plt.imshow(resized_img)\n",
    "resized_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also manipulate the pixels to rotate image by 90° counter-clockwise.\n",
    "rotated_img = transform.rotate(img, 90, resize=True)\n",
    "plt.imshow(rotated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Resize the image by a factor 3 and rotate the image by 45° counter-clockwise.\n",
    "# Re-use the code from the cells above, or write your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "Convolutions are another way to manipulate the pixel values of images. You can imagine the mathemetical operation of a convolution (https://en.wikipedia.org/wiki/Convolution) as a small screen, known as a kernel, running over the image. The kernel changes the value of the pixel in the middle based on the values of the pixels surrounding it. As we will see below, convolutions highlight certain features of an image. Convolutional Neural Networks use convolutions to extract features --- complicated patterns in pixel values. Below we apply several convolutions to see which features they highlight. \n",
    "\n",
    "<br><br><br><img src=\"https://community.arm.com/cfs-file/__key/communityserver-blogs-components-weblogfiles/00-00-00-20-66/4786.conv.png\" /><p style=\"text-align:center;\">\n",
    "An example of a convolution filter.  Each transformed pixel value is created by multiplying its current value and the values of the pixels around it against a matrix of coefficients\n",
    "</p>\n",
    "<p style=\"text-align:center;\">\n",
    "(borrowed from [Embarrassingly Parallel](https://community.arm.com/graphics/b/blog/posts/when-parallelism-gets-tricky-accelerating-floyd-steinberg-on-the-mali-gpu))</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell loads the necessary libraries\n",
    "from scipy.ndimage.filters import convolve\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.signal import convolve2d\n",
    "from scipy import misc\n",
    "from skimage import filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blur (Guassian Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we initialize a matrix of 100 x 100 pixels with a value zero. \n",
    "# And we also add a pixel of value 1 at position 50, 50.\n",
    "x = np.zeros((100, 100))\n",
    "x[50, 50] = 1\n",
    "plt.imshow(x, interpolation='none', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Filter\n",
    "\n",
    "We can use a Gaussian filter to add a blurring effect to an image. The filter smoothens the transition between the zerores and the 1 by adding intermediate values. The sigma determines the slope of the filter, and thereby the smoothness of the transition.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Gaussian_Filter.svg/700px-Gaussian_Filter.svg.png\" width=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we apply to filter to the matrix X and save it as a variable Y. \n",
    "## Change the value of sigma and describe what happens\n",
    "\n",
    "y = ndi.filters.gaussian_filter(x, sigma=5)\n",
    "plt.imshow(y, interpolation='none', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell applies a Guassian Filter to an image of the band Blur.\n",
    "# QUESTION: Which features are highlighted by a Guassian Filter? \n",
    "blur = io.imread(join(\"images\", \"blur.jpg\"), as_gray=True)\n",
    "blurred_blur = ndi.filters.gaussian_filter(blur, sigma=5)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Blur')\n",
    "plt.imshow(blur, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Blurred Blur')\n",
    "plt.imshow(blurred_blur, cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobel Operator\n",
    "The sobel operator can be used to highlight edges in an image. \n",
    "For more on the operator: https://en.wikipedia.org/wiki/Sobel_operator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we initialize two kernels. \n",
    "# One for horizontal edge detection (h_kernel) and one for vertical edge detection (v_kernel)\n",
    "\n",
    "h_kernel = np.array([[ 1,  2,  1],\n",
    "                     [ 0,  0,  0],\n",
    "                     [-1, -2, -1]])\n",
    "\n",
    "v_kernel = np.array([[ -1,  0,  1],\n",
    "                     [ -2,  0,  2],\n",
    "                     [-1, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the kernels\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Horizontal Edge Detection (h_kernel)')\n",
    "plt.imshow(h_kernel, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Vertical Edge Detection (v_kernel)')\n",
    "plt.imshow(v_kernel, cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the image of the train as a grayscale image\n",
    "\n",
    "grayscale_train = io.imread(join(\"images\", \"color_train.jpg\"), as_gray=True)\n",
    "plt.imshow(grayscale_train, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Sobel operator\n",
    "Gx = convolve2d(grayscale_train, h_kernel)\n",
    "Gy = convolve2d(grayscale_train, v_kernel)\n",
    "G = np.sqrt(Gx**2 + Gy**2)\n",
    "plt.imshow(G, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your own Convolutional Neural Network\n",
    "\n",
    "Convolution Neural Networks are able to detect patterns in pixel values. They do this by learning (Artificial Intelligence) significant similarities and differences in a collection of images. As we will see below, convolutions play an important role in the architecture, the specific sequence of different layers, of a convolutional neural network. \n",
    "\n",
    "We will start by building our own classifier. On of the first convolutional neural networks was used to recognize handwritten digitis. The network was trained on data from the MNIST dataset, which contains 60,000 training and 10,000 testing images. We use Keras, a open-source Python framework for building neural networks to start building our own convolutional neural network.\n",
    "\n",
    "If you feel you do not understand how convolutional neural networks work consider watching <a href=\"https://www.youtube.com/watch?v=CsN-qwpF8Bc\" /> this short (somewhat embrassing) movie </a> I (Thomas) made for the Universiteit van Nederland (Dutch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the MNIST dataset\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the training and testing images.\n",
    "# QUESTION What is the difference between X_train and Y_train? \n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data('/tmp/mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the shape (structure) of the X_train and X_Test set.\n",
    "# QUESTION What do these values indicate?\n",
    "\n",
    "print(\"Shape of X_Train: \", X_train.shape)\n",
    "print(\"Shape of X_Test: \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the first entry in the X_train dataset\n",
    "# QUESTION: Which number do you have to fill in if you would want to print the 10th entry? Why?\n",
    "\n",
    "X_train[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the images of the first entry in the X_train set. \n",
    "plt.imshow(X_train[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshapes the images of the X_train set to 28X28 pixels. \n",
    "# QUESTION: Think of kernel going over the image (especially its edges). Why do we reshape the images into squares?\n",
    "\n",
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the new shape of the X_train set.\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the numbers of the dataset from integers to floats . \n",
    "# WHY DO WE DO THIS?\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports a function for creating a dummy encoding. \n",
    "# http://www.statisticssolutions.com/dummy-coding-the-how-and-why/\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we see the annotated label (y_train[0]) for the first image (X_train[0]), which resembled a five.\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we turn the y variable into dummy variables. \n",
    "# go to 10 binary columns\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Y_train: \", y_train_cat.shape)\n",
    "print(\"Shape of Y_test: \", y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the dummy encoding for the first entry. \n",
    "# QUESTION What number does this signify? \n",
    "y_train_cat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION What number does this array signify?\n",
    "y_train_cat[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports several functions from the Keras library.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import MaxPool2D, AvgPool2D, Conv2D\n",
    "from keras.layers import Flatten, Activation\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we initialize the architecture of our network. It consists of different kinds of layers. \n",
    "We already know the convolutional layer: our model has 32 filters in one layer.\n",
    "\n",
    "### One Convolution layer\n",
    "<img src=\"https://i.stack.imgur.com/xdXTn.gif\">\n",
    "\n",
    "### One Max Pooling layer\n",
    " (Max)Pooling layers reduce the complexity of the information of the image. \n",
    "<img src=\"https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1))) #convolution layer 32 filters (3 x 3 pixels), output 26x26\n",
    "model.add(MaxPool2D(pool_size=(2, 2))) # pooling layer to reduce size output is 13x13 \n",
    "model.add(Activation('relu')) \n",
    "\n",
    "model.add(Flatten()) #flatten tensor into long array\n",
    "\n",
    "model.add(Dense(128, activation='relu')) \n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell describes the architecture of our network (model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we train the initialized model on the trained data of MNIST (the hand-written digits)\n",
    "\n",
    "h = model.fit(X_train, y_train_cat, batch_size=128,\n",
    "          epochs=3, verbose=1, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we evaluate the model against the test data\n",
    "\n",
    "plt.plot(h.history['acc'])\n",
    "plt.plot(h.history['val_acc'])\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "test_accuracy = model.evaluate(X_test, y_test_cat)[1]\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are going to test whether our network works. \n",
    "# The label of the 151st entry is a four. \n",
    "y_train[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's ask our model to predict the value of the 151st entry. \n",
    "\n",
    "model.predict_classes(X_train[150].reshape(-1, 28, 28, 1))[0]\n",
    "\n",
    "# Try some other entries yourselves! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification\n",
    "\n",
    "Convolutional neural networks are often used for image and/or object classification. The task of networks is to flag an image in a set if it contains a specific object, scene, colour scheme etc etc (image classification) or detect and 'box' a specific number of predetermined object categories (see image below)\n",
    "<br><br><br><img src=\"https://www.pyimagesearch.com/wp-content/uploads/2017/09/example03_result.jpg\" /><p style=\"text-align:center;\"> A person riding a horse and two potted plants are successfully identified despite a lot of objects in the image via deep learning-based object detection\n",
    "</p>\n",
    "<p style=\"text-align:center;\">\n",
    "(borrowed from [Object detection with deep learning](https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/))</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads functions from Keras library\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input, decode_predictions\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using VGG19 for image classification\n",
    "\n",
    "The VGG19 model was trained to identify 1000 classes of objects within an image. It was built as part of the ImageNet challenge, one of the most influential computer vision competitions that has been running since 2010.\n",
    "\n",
    "\n",
    "<img src=\"https://lihan.me/assets/images/vgg-hero-cover.jpg\"/>\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "__VGG19 Architecture__\n",
    "</p>\n",
    "<p style=\"text-align:center;\">\n",
    "(borrowed from [vgg19-caltech101-classification](https://lihan.me/2018/01/vgg19-caltech101-classification/))\n",
    "</p>\n",
    "\n",
    "VGG models described in the [original paper](https://arxiv.org/pdf/1409.1556.pdf) are trained with images whose size is 224x224x3. This can not be changed, so the input image for the transfer learning task should have the same image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the VGG19 model trained on the imagenet data. \n",
    "vgg19_full = VGG19(weights='imagenet')\n",
    "vgg19_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads an image of a 'bloemkool' from the folder 'images'\n",
    "# QUESTION: Can you load your own image?\n",
    "img_path = join(\"images\", \"bloemkool.jpg\")\n",
    "img = image.load_img(img_path, target_size=(224, 224)) #image needs to be 224 x 224\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell uses VGG19 to predict what the image shows.\n",
    "# QUESTION: Can you predict the category of a random image from the internet using this notebook?\n",
    "prediction = vgg19_full.predict(x)\n",
    "print(prediction.shape)\n",
    "for pred in decode_predictions(prediction)[0]:\n",
    "        print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your own classifier\n",
    "\n",
    "We are going to train a model that can identify \"bakfietsen\".\n",
    "\n",
    "<img src=\"https://images4.persgroep.net/rcs/BA5IZl03LajJ8m9wiriELowxFps/diocontent/68923602/_crop/0/135/1580/893/_fitwidth/694/?appId=21791a8992982cd8da851550a453bd7f&quality=0.9\">\n",
    "\n",
    "Here we load a different model, namely inception V3, which was developed by Google. Inception is faster than VGG19 when training your own classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO: ADD MORE IMAGES (SMALLER) TO IMPROVE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the architecture, with pre-trained parameters from ImageNet\n",
    "# but without the final (top) layer of object classification\n",
    "\n",
    "inception_model = inception_v3.InceptionV3(include_top=False, pooling='avg', weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img):\n",
    "    x = np.expand_dims(img.astype(np.float32), axis=0)  # Make a batch of 1 image by expanding the first dimension\n",
    "    x = inception_v3.preprocess_input(x)  # Preprocess the input image ([0,255] range to [-1,+1])\n",
    "    return inception_model.predict(x)[0]  # Make the batch goes through the network and convert the batch back to single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Here we load our training, test, and random images into a list variable. \n",
    "\n",
    "\n",
    "# Gather the image files in each directory\n",
    "filenames_bakfiets = glob('data/bakfiets/*.jpg')\n",
    "print(\"Number of training positive files : {}\".format(len(filenames_bakfiets)))\n",
    "filenames_random = glob('data/random/*.jpg')\n",
    "print(\"Number of training negative files : {}\".format(len(filenames_random)))\n",
    "filenames_test = glob('data/test/*.jpg')\n",
    "print(\"Number of test files : {}\".format(len(filenames_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "# Extracting features for the positive images\n",
    "for filename in tqdm(filenames_bakfiets, desc='Computing features for bakfiets images'):\n",
    "    img = io.imread(filename)\n",
    "    # Add the CNN features to the list\n",
    "    features.append(extract_features(img))\n",
    "    # Add the positive label to the list\n",
    "    labels.append(1.)\n",
    "# Extracting features for the negative images\n",
    "for filename in tqdm(filenames_random, desc='Computing features for random images'):\n",
    "    img = io.imread(filename)\n",
    "    # Add the CNN features to the list\n",
    "    features.append(extract_features(img))\n",
    "    # Add the negative label to the list\n",
    "    labels.append(0.)\n",
    "features = np.stack(features)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images: \", features.shape[0])\n",
    "print(\"Number of features per image: \", features.shape[1])\n",
    "print(\"\\n\")\n",
    "print(\"Features of image 1: \", features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of labels, 1 signifies a bakfiets, 0 signifies something different. \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now going to train a classifier that can predict (based on the features) \n",
    "# whether an image is a bakfiets (1) or something different (0)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate a classifier that will predict a probability\n",
    "classifier = SVC(probability=True, kernel='linear')\n",
    "# Train the binary classifier from the extracted features and telling it which images are the good ones with the labels\n",
    "classifier.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each image in the directory with test data\n",
    "for filename in tqdm(filenames_test, desc=\"Predicting\"):\n",
    "    # Load the image\n",
    "    img = io.imread(filename)\n",
    "    # Extract the features\n",
    "    feat = extract_features(img)\n",
    "    # Get the probabilities for the computed features\n",
    "    probs = classifier.predict_proba(np.expand_dims(feat, axis=0))[0]\n",
    "    prob_bakfiets = probs[1]\n",
    "    # Display the results\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Probability : {:.01f}%\".format(100*prob_bakfiets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face detection\n",
    "\n",
    "An important task of computer vision is face detecting. In the past, face detection was achieved by recognizing the relative distance between 'facial markers'. In recent years, machine learning has also greatly improved the task of face detection.<br><br><br><img src=\"https://i.dailymail.co.uk/i/pix/2015/02/18/25CCD2F400000578-2958597-image-a-27_1424270103152.jpg\" /><p style=\"text-align:center;\">\n",
    "__Face Detection__\n",
    "</p>\n",
    "<p style=\"text-align:center;\">\n",
    "(Stolen from [A British tabloid](https://pleasedonotreportus.com/))\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the face_recognition library\n",
    "import face_recognition as fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image face.jpg from the folder 'faces'.\n",
    "img_path = join('images', 'faces', 'face.jpg')\n",
    "img = fr.load_image_file(img_path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we extract the locations of the faces and store them in the variable face_locations\n",
    "face_locations = fr.face_locations(img, number_of_times_to_upsample=0, model='cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we use the location information stored in face_locations to draw rectangles on top of the image.\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "plt.imshow(img)\n",
    "n, m, d = img.shape\n",
    "for face in face_locations:\n",
    "    rect = plt.Rectangle((face[3], face[0]), face[2] - face[0], face[1] - face[3],\n",
    "                         edgecolor='orange', linewidth=2, facecolor='none')\n",
    "    ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Thank you! \n",
    "\n",
    "If you have any questions please email us or visit our websites.\n",
    "\n",
    "<a href=\"https://melvinwevers.github.io/\" />Melvin Wevers </a> - melvin.wevers@dh.huc.knaw.nl <br>\n",
    "<a href=\"http://illustratednewspictures.tumblr.com/\" /> Thomas Smits </a> - thomassmits@hotmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_course",
   "language": "python",
   "name": "cv_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
